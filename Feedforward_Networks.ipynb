{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c4728ba9-bb59-4cb9-81e8-b7d112c74038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732a1fd-1281-48b4-adf9-6d22237f75f0",
   "metadata": {},
   "source": [
    "### Creating a Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a7130e5-fafc-4a9e-b951-7d186fd8b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1 + np.exp(-x)) \n",
    "    # the activation function here is a sigmoid function, but we can even use functions like the binary step or linear function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6e40b431-447c-4ad9-ad49-b200f75aebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:\n",
    "    def __init__(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "    def feedforward(self, inp):\n",
    "        output = np.dot(self.weight, inp) + self.bias\n",
    "        return activation(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ffb4091a-e1cb-4029-8ea7-03471c6032e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999998874648379\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "weight = np.array([1, 2])\n",
    "bias = 5\n",
    "n = neuron(weight, bias)\n",
    "\n",
    "inp = np.array([3, 4])\n",
    "print(n.feedforward(inp)) # neuron output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05540a7f-de80-4318-8ee4-112863a52c51",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "496fbdce-6ce2-4e82-8729-9ede671f20f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996646497563555\n"
     ]
    }
   ],
   "source": [
    "class network:\n",
    "    def __init__(self):\n",
    "        weight = np.array([1, 2])\n",
    "        bias = 5\n",
    "\n",
    "        self.h1 = neuron(weight, bias)\n",
    "        self.h2 = neuron(weight, bias)\n",
    "        self.o1 = neuron(weight, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "        return out_o1\n",
    "\n",
    "neural_network = network()\n",
    "x = np.array([3, 4])\n",
    "print(neural_network.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb8195-9453-43dd-8c57-ce6f86035efd",
   "metadata": {},
   "source": [
    "### Mean Squared Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "850f4992-1d51-4c4a-8186-c39f6c7c023e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def mse(y_exp, y_act):\n",
    "    mean_squared_error = ((y_act - y_exp)**2).mean()\n",
    "    return mean_squared_error\n",
    "\n",
    "y_act = np.array([1, 0, 0, 1])\n",
    "y_exp = np.array([0, 0, 0, 0])\n",
    "\n",
    "print(mse(y_act, y_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01043e59-9c17-4790-b6f8-63255cddfdc5",
   "metadata": {},
   "source": [
    "### Applying Stochastic Gradient Descent to Minimise Loss\n",
    "\n",
    "$w_1 \\leftarrow w_1 - \\eta \\frac{\\partial L}{\\partial w_1}$ where $\\eta$ is the learning rate (how fast we train)\n",
    "\n",
    "This is simple trial network to predict sex based on height and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a70a49b-cfa0-4b2e-b3a2-359fca9c0500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_act(x):\n",
    "    y = activation(x)\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "755ef8e2-5f8a-434f-b965-e197d8f58eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class new_network:\n",
    "    def __init__(self):\n",
    "        self.w1 = np.random.normal() # random samples from a Gaussian distribution\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "\n",
    "        self.b1 = np.random.normal() # for h1\n",
    "        self.b2 = np.random.normal() # for h2\n",
    "        self.b3 = np.random.normal() # for o1\n",
    "\n",
    "    def ff(self, x):\n",
    "        h1 = activation(self.w1 * x[0] + self.w2 * x[1] + self.b1) # numpy array with two elements\n",
    "        h2 = activation(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = activation(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_act):\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000 # number of times to loop through entire dataset\n",
    "        for i in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_act):\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = activation(sum_h1)\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = activation(sum_h2)\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = activation(sum_o1)\n",
    "                y_pred = o1\n",
    "\n",
    "                # partial derivatives\n",
    "                dL_dypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # h1\n",
    "                dh1_dw1 = x[0] * der_act(sum_h1)\n",
    "                dh1_dw2 = x[1] * der_act(sum_h1)\n",
    "                dh1_db1 = der_act(sum_h1)\n",
    "\n",
    "                # h2\n",
    "                dh2_dw3 = x[0] * der_act(sum_h2)\n",
    "                dh2_dw4 = x[1] * der_act(sum_h2)\n",
    "                dh2_db2 = der_act(sum_h2)\n",
    "\n",
    "                # o1\n",
    "                dypred_dw5 = h1 * der_act(sum_o1)\n",
    "                dypred_dw6 = h2 * der_act(sum_o1)\n",
    "                dypred_db3 = der_act(sum_o1)\n",
    "                \n",
    "                # for backprop\n",
    "                dypred_dh1 = self.w5 * der_act(sum_o1)\n",
    "                dypred_dh2 = self.w6 * der_act(sum_o1)\n",
    "\n",
    "                # applying SGD to change weights and biases\n",
    "                self.w1 -= learn_rate * dL_dypred * dypred_dh1 * dh1_dw1\n",
    "                self.w2 -= learn_rate * dL_dypred * dypred_dh1 * dh1_dw2\n",
    "                self.b1 -= learn_rate * dL_dypred * dypred_dh1 * dh1_db1\n",
    "\n",
    "                self.w3 -= learn_rate * dL_dypred * dypred_dh2 * dh2_dw3\n",
    "                self.w4 -= learn_rate * dL_dypred * dypred_dh2 * dh2_dw4\n",
    "                self.b2 -= learn_rate * dL_dypred * dypred_dh2 * dh2_db2\n",
    "\n",
    "                self.w5 -= learn_rate * dL_dypred * dypred_dw5\n",
    "                self.w6 -= learn_rate * dL_dypred * dypred_dw6\n",
    "                self.b3 -= learn_rate * dL_dypred * dypred_db3\n",
    "\n",
    "                # calculating loss after avery 10 epochs\n",
    "                if i % 10 == 0:\n",
    "                    pred_y = np.apply_along_axis(self.ff, 1, data)\n",
    "                    loss = mse(all_y_act, pred_y)\n",
    "                    print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5af616dc-8669-47d8-954d-c3cd939a804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3009861299783996\n",
      "0 0.3223210884546784\n",
      "0 0.321611382465811\n",
      "0 0.32164349362208405\n",
      "10 0.29528993437435597\n",
      "10 0.2955527901771161\n",
      "10 0.29389846820825194\n",
      "10 0.2944522254121016\n",
      "20 0.2819795994596552\n",
      "20 0.2822682072880797\n",
      "20 0.28069525053092564\n",
      "20 0.2813539610477761\n",
      "30 0.27157915414229356\n",
      "30 0.2718773880452135\n",
      "30 0.2705592182472301\n",
      "30 0.2712229456927708\n",
      "40 0.264103405777873\n",
      "40 0.26440726018097566\n",
      "40 0.2633987335946388\n",
      "40 0.26398604364025685\n",
      "50 0.25905408127001117\n",
      "50 0.25934644939501056\n",
      "50 0.25861145744909014\n",
      "50 0.25908929949101894\n",
      "60 0.2557641345657295\n",
      "60 0.2560268771731857\n",
      "60 0.2555014500664862\n",
      "60 0.25587216784548866\n",
      "70 0.25366232401965283\n",
      "70 0.25388425913400997\n",
      "70 0.25351087175266657\n",
      "70 0.25379070799670156\n",
      "80 0.25233363448096613\n",
      "80 0.2525113907545729\n",
      "80 0.25224718988082323\n",
      "80 0.2524541132726916\n",
      "90 0.25149762225933053\n",
      "90 0.2516332454357243\n",
      "90 0.25144816094514105\n",
      "90 0.2515976816036847\n",
      "100 0.2509717719211757\n",
      "100 0.2510700703015732\n",
      "100 0.25094315414479507\n",
      "100 0.25104768094945523\n",
      "110 0.25063986616433626\n",
      "110 0.2507065503717209\n",
      "110 0.25062300363580603\n",
      "110 0.25069226040164466\n",
      "120 0.2504288429908683\n",
      "120 0.2504694896460099\n",
      "120 0.2504186455044852\n",
      "120 0.2504602374101638\n",
      "130 0.25029307669372813\n",
      "130 0.2503126576042538\n",
      "130 0.25028667716056047\n",
      "130 0.25030657091850445\n",
      "140 0.25020413409612174\n",
      "140 0.25020686747781506\n",
      "140 0.2501998945564221\n",
      "140 0.25020278754372854\n",
      "150 0.250144257572411\n",
      "150 0.25013361956019525\n",
      "150 0.2501412195566928\n",
      "150 0.25013082047034085\n",
      "160 0.2501022719145912\n",
      "160 0.25008107617503894\n",
      "160 0.25009985117376454\n",
      "160 0.25007909669215544\n",
      "170 0.2500710224189345\n",
      "170 0.25004152014896425\n",
      "170 0.2500688353057107\n",
      "170 0.2500400610925105\n",
      "180 0.25004575609536006\n",
      "180 0.25000973361431056\n",
      "180 0.25004351444282136\n",
      "180 0.2500085938365481\n",
      "190 0.25002305873699193\n",
      "190 0.2499819224275519\n",
      "190 0.2500204955675706\n",
      "190 0.24998095695848027\n",
      "200 0.25000007326397233\n",
      "200 0.2499549164014757\n",
      "200 0.2499968709418309\n",
      "200 0.24995400559554543\n",
      "210 0.24997374705255707\n",
      "210 0.24992539093251903\n",
      "210 0.24996943218153128\n",
      "210 0.2499244102707574\n",
      "220 0.24993971926699873\n",
      "220 0.24988870476382355\n",
      "220 0.24993344621305572\n",
      "220 0.24988747818380075\n",
      "230 0.24988983025870867\n",
      "230 0.24983626436802614\n",
      "230 0.24987980791325365\n",
      "230 0.24983445152504002\n",
      "240 0.24980436403738415\n",
      "240 0.24974713989723227\n",
      "240 0.24978588778405247\n",
      "240 0.24974384241671949\n",
      "250 0.24961710476044127\n",
      "250 0.24954882644011578\n",
      "250 0.2495725564977338\n",
      "250 0.2495404404911148\n",
      "260 0.2489121907100847\n",
      "260 0.24874388369502187\n",
      "260 0.24869850419712541\n",
      "260 0.24869638788111878\n",
      "270 0.2438556347387436\n",
      "270 0.24307740647795845\n",
      "270 0.24311405673057562\n",
      "270 0.2431258896076761\n",
      "280 0.24135884582457187\n",
      "280 0.24018669426875575\n",
      "280 0.2406576863448298\n",
      "280 0.2402951662427343\n",
      "290 0.23871090441440623\n",
      "290 0.2370553686253719\n",
      "290 0.23807221617285146\n",
      "290 0.23725743595558885\n",
      "300 0.23565288080750868\n",
      "300 0.23363797609091486\n",
      "300 0.23525563871665847\n",
      "300 0.23398528188594475\n",
      "310 0.23213258784446877\n",
      "310 0.229881343933148\n",
      "310 0.23209784371056538\n",
      "310 0.23046940232783814\n",
      "320 0.22811990664374684\n",
      "320 0.22573229732979708\n",
      "320 0.228425541759743\n",
      "320 0.22659797257494846\n",
      "330 0.22360796115068488\n",
      "330 0.22122269226665897\n",
      "330 0.2241469566505217\n",
      "330 0.22227193796780997\n",
      "340 0.21872758062832343\n",
      "340 0.2165061497916425\n",
      "340 0.21939603394417018\n",
      "340 0.21758208061268502\n",
      "350 0.2137728854555067\n",
      "350 0.21181782348038908\n",
      "350 0.21450573436737827\n",
      "350 0.21281097456069678\n",
      "360 0.2090314853262786\n",
      "360 0.20735754091564512\n",
      "360 0.20979134499472735\n",
      "360 0.2082331071337497\n",
      "370 0.20464758568314323\n",
      "370 0.20322137597945342\n",
      "370 0.20541080809877632\n",
      "370 0.20398362793719182\n",
      "380 0.20064782197622524\n",
      "380 0.1994261868779073\n",
      "380 0.2013995189945735\n",
      "380 0.2000896591297937\n",
      "390 0.1970066882581988\n",
      "390 0.1959514311616656\n",
      "390 0.19773810116259863\n",
      "390 0.19653076603039038\n",
      "400 0.19368336261019065\n",
      "400 0.19276378312740544\n",
      "400 0.19438968564611636\n",
      "400 0.1932716183152568\n",
      "410 0.19063639580403308\n",
      "410 0.18982835387693037\n",
      "410 0.19131522188125755\n",
      "410 0.19027529370301388\n",
      "420 0.18782871291414377\n",
      "420 0.18711320720579083\n",
      "420 0.1884790647418399\n",
      "420 0.18750814148499612\n",
      "430 0.18522881856975404\n",
      "430 0.18459077764428183\n",
      "430 0.18585057919006043\n",
      "430 0.18494118456733294\n",
      "440 0.18281056231818615\n",
      "440 0.18223792323507876\n",
      "440 0.18340414284913176\n",
      "440 0.18255011217467698\n",
      "450 0.18055241861542862\n",
      "450 0.1800354398845981\n",
      "450 0.18111855967811763\n",
      "450 0.18031474499584502\n",
      "460 0.17843667504146854\n",
      "460 0.17796742304032515\n",
      "460 0.17897632315294365\n",
      "460 0.17821836081247494\n",
      "470 0.17644868424224014\n",
      "470 0.17602064583370008\n",
      "470 0.17696291131601632\n",
      "470 0.17624704547257392\n",
      "480 0.17457623060367417\n",
      "480 0.17418401804886866\n",
      "480 0.17506617935972\n",
      "480 0.1743891303954609\n",
      "490 0.1728090174061191\n",
      "490 0.1724481415026337\n",
      "490 0.17327586320990176\n",
      "490 0.17263473040951188\n",
      "500 0.17113826215545733\n",
      "500 0.1708049563661519\n",
      "500 0.17158318584890703\n",
      "500 0.17097537547015984\n",
      "510 0.169556382223092\n",
      "510 0.16924746529361095\n",
      "510 0.1699805506618622\n",
      "510 0.16940372253163966\n",
      "520 0.1680567528677906\n",
      "520 0.1677695207502506\n",
      "520 0.16846130506047025\n",
      "520 0.16791333255968852\n",
      "530 0.16663352182696767\n",
      "530 0.16636566209139048\n",
      "530 0.16701955924969122\n",
      "530 0.16649849892737553\n",
      "540 0.1652814673954342\n",
      "540 0.16503099101827426\n",
      "540 0.16565004746573364\n",
      "540 0.16515411556733056\n",
      "550 0.1639958895663633\n",
      "550 0.16376107623420108\n",
      "550 0.16434802152373804\n",
      "550 0.1638755754913286\n",
      "560 0.16277252612430632\n",
      "560 0.16255188011325183\n",
      "560 0.16310916874453701\n",
      "560 0.1626586923086767\n",
      "570 0.16160748748463233\n",
      "570 0.1613997018634407\n",
      "570 0.1619295481836892\n",
      "570 0.16149963907095571\n",
      "580 0.1604972055870061\n",
      "580 0.16030113300816984\n",
      "580 0.16080554056677065\n",
      "580 0.1603949001355499\n",
      "590 0.15943839332546406\n",
      "590 0.15925302205889602\n",
      "590 0.15973380848779234\n",
      "590 0.1593412328091477\n",
      "600 0.15842801189517844\n",
      "600 0.15825244605607067\n",
      "600 0.1587112643092844\n",
      "600 0.15833563635356201\n",
      "610 0.15746324411336243\n",
      "610 0.1572966872630214\n",
      "610 0.1577350438680199\n",
      "610 0.15737532655846082\n",
      "620 0.1565414722782549\n",
      "620 0.15638321375147873\n",
      "620 0.1568024845875446\n",
      "620 0.15645771455216897\n",
      "630 0.1556602595060846\n",
      "630 0.15550966295374125\n",
      "630 0.1559111069672559\n",
      "630 0.15558038886870476\n",
      "640 0.15481733376339232\n",
      "640 0.15467382750377956\n",
      "640 0.1550585986892865\n",
      "640 0.15474110004563196\n",
      "650 0.15401057401595666\n",
      "650 0.15387364287039335\n",
      "650 0.1542428007834904\n",
      "650 0.15393774721585213\n",
      "660 0.15323799806484945\n",
      "660 0.15310717641710586\n",
      "660 0.15346169543617783\n",
      "660 0.15316836629457042\n",
      "670 0.15249775174919422\n",
      "670 0.15237261761888\n",
      "670 0.15271339513411292\n",
      "670 0.15243111946357674\n",
      "680 0.15178809927475823\n",
      "680 0.15166826923471402\n",
      "680 0.15199613291226172\n",
      "680 0.1517242857285934\n",
      "690 0.15110741448553722\n",
      "690 0.15099253928498246\n",
      "690 0.151308253529734\n",
      "690 0.15104625237913505\n",
      "700 0.15045417293786634\n",
      "700 0.1503439337183377\n",
      "700 0.15064820543906818\n",
      "700 0.15039550721952413\n",
      "710 0.1498269446676076\n",
      "710 0.1497210496789667\n",
      "710 0.15001453344369253\n",
      "710 0.1497706314684048\n",
      "720 0.14922438756376577\n",
      "720 0.149122569303838\n",
      "720 0.1494058719601425\n",
      "720 0.14917029324519887\n",
      "730 0.1486452412787456\n",
      "730 0.1485472539932981\n",
      "730 0.14882093881763858\n",
      "730 0.1485932415775553\n",
      "740 0.14808832161803215\n",
      "740 0.14799393910846712\n",
      "740 0.14825852953954116\n",
      "740 0.14803830087550246\n",
      "750 0.14755251536154862\n",
      "750 0.14746152905636895\n",
      "750 0.14771751206016115\n",
      "750 0.14750436582679863\n",
      "760 0.14703677547617042\n",
      "760 0.146948992729378\n",
      "760 0.1471968218372281\n",
      "760 0.14699039667470068\n",
      "770 0.1465401166844694\n",
      "770 0.14645535926988945\n",
      "770 0.14669545732561506\n",
      "770 0.146495414844593\n",
      "780 0.1460616113591738\n",
      "780 0.14597971413450384\n",
      "780 0.14621247578209223\n",
      "780 0.14601849889004545\n",
      "790 0.1456003857163639\n",
      "790 0.14552119543472852\n",
      "790 0.14574698937424585\n",
      "790 0.1455587807321932\n",
      "800 0.1451556162833178\n",
      "800 0.14507899053341317\n",
      "800 0.14529816156945974\n",
      "800 0.14511544216906475\n",
      "810 0.144726526619328\n",
      "810 0.14465233287800228\n",
      "810 0.14486520378217782\n",
      "810 0.14468771163376482\n",
      "820 0.14431238426985535\n",
      "820 0.14424049905327851\n",
      "820 0.1444473722596329\n",
      "820 0.1442748611823707\n",
      "830 0.14391249793614802\n",
      "830 0.14384280603766395\n",
      "830 0.14404396518795662\n",
      "830 0.14387620369407958\n",
      "840 0.14352621484398925\n",
      "840 0.14345860864838347\n",
      "840 0.14365432000208472\n",
      "840 0.1434910902676314\n",
      "850 0.1431529182966071\n",
      "850 0.1430872971619025\n",
      "850 0.14327781088422525\n",
      "850 0.14311890779933756\n",
      "860 0.14279202539799085\n",
      "860 0.1427282950970643\n",
      "860 0.14291384643686722\n",
      "860 0.14275907672923177\n",
      "870 0.14244298493396126\n",
      "870 0.14238105714927296\n",
      "870 0.14256186751740513\n",
      "870 0.14241104894291784\n",
      "880 0.14210527539933446\n",
      "880 0.14204506726491845\n",
      "880 0.1422213452224534\n",
      "880 0.14207430581766134\n",
      "890 0.1417784031604297\n",
      "890 0.1417198368460263\n",
      "890 0.14189177901084377\n",
      "890 0.14174835640215394\n",
      "900 0.14146190074299625\n",
      "900 0.14140490307583867\n",
      "900 0.14157269495513783\n",
      "900 0.14143273572018958\n",
      "910 0.14115532523640145\n",
      "910 0.1410998273567114\n",
      "910 0.14126364411225817\n",
      "910 0.1411270031892344\n",
      "920 0.14085825680561598\n",
      "920 0.14080419385233306\n",
      "920 0.14096420100455992\n",
      "920 0.14083074114555816\n",
      "930 0.14057029730318016\n",
      "930 0.14051760812685687\n",
      "930 0.1406739622033152\n",
      "930 0.1405435534682245\n",
      "940 0.14029106897392518\n",
      "940 0.1402396958740729\n",
      "940 0.1403925450071955\n",
      "940 0.1402650642948176\n",
      "950 0.14002021324577232\n",
      "950 0.13997010173025065\n",
      "950 0.14011958620889253\n",
      "950 0.13999491682232207\n",
      "960 0.13975738960043482\n",
      "960 0.13970848816474774\n",
      "960 0.13985474094353878\n",
      "960 0.13973277218706437\n",
      "970 0.13950227451831426\n",
      "970 0.13945453444290967\n",
      "970 0.13959768161306502\n",
      "970 0.13947830841808473\n",
      "980 0.13925456049231177\n",
      "980 0.139207935656188\n",
      "980 0.13934809688107297\n",
      "980 0.13923121945872563\n",
      "990 0.13901395510567063\n",
      "990 0.13896840181477466\n",
      "990 0.13910569073320883\n",
      "990 0.13899121425161887\n"
     ]
    }
   ],
   "source": [
    "# weights (not the neural networks weight) & heights: [17, 22, 62, 49], [110, 122, 156, 145]\n",
    "data = np.array([[-21, -23],\n",
    "                 [-16, -11],\n",
    "                 [24, 23],\n",
    "                 [11, 12]])\n",
    "all_y_act = np.array([1, 0, 0, 1]) # 1 is female and 0 is male\n",
    "nn = new_network()\n",
    "nn.train(data, all_y_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e359584f-3c33-480d-bde5-43064311e4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47010075968504694\n",
      "0.9592375447242665\n"
     ]
    }
   ],
   "source": [
    "random_1 = np.array([16, 18]) # 54, 151 --> offset: 38\n",
    "random_2 = np.array([37, 26]) # 75, 159 --> offset: 133\n",
    "print(nn.ff(random_1)) # female\n",
    "print(nn.ff(random_2)) # male"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
